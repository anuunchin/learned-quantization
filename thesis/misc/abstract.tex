\thispagestyle{empty}
\vspace*{1.0cm}
%\vspace*{\fill}

\begin{center}
    \textbf{Abstract} \label{abstract}
\end{center}

\vspace*{0.5cm}

%\hspace*{0.5em}% State the problem
\noindent Machine learning (ML) models are notoriously resource-intensive.
Given their widespread application on end-user devices,
the need to reduce the memory and computing requirements of such models is becoming ever more pressing.
% Say why it's an interesting problem
Interestingly, the very redundancy that contributes to the inefficient nature of predictive models, 
offers the remedy to the malady of resource-intensiveness by creating opportunities for exploitation.
One way to exploit the redundancy of Neural Networks (NNs) is to 
\textit{quantize} them â€” 
to approximate their full-precision representations with lower-bit-width counterparts.
Among the various quantization approaches, 
\textit{learned quantization} presents itself as a promising research area with 
room for contribution, considering the vast landscape of opportunities for
ML to autonomously learn their optimal quantization strategies.
% Say what your solution achieves
Hence, this work introduces two techniques for learned quantization,
achieving effective model compression without incurring significant accuracy degradation.
The first technique involves custom regularization terms that directly take into account
quantization goals, the second novel approach incorporates a custom scaling factor gradient calculation that utilizes 
the gradient of the parameters that are being quantized.
As a result, we achieve a memory usage reduction of up to \( 8 \times \) is achieved for the model weights
trained on MNIST, CIFAR-10, and Imagenette, a simplified ImageNet dataset. 
