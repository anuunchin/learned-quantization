\thispagestyle{empty}
\vspace*{1.0cm}

\begin{center}
    \textbf{Abstract} \label{abstract}
\end{center}

\vspace*{0.5cm}

\noindent Machine learning (ML) models are notoriously resource-intensive.
Given their widespread application across every-day edge devices,
the need to reduce their memory and computing requirements is becoming ever more pressing.
Despite the said resource-intensiveness of ML models, at the same time 
they offer the main ingredient for the remedy to the malady - redundancy - 
which can be exploited to reduce their memory usage.
While the redundancy exploitation of ML models is already a common 
technique that comes in different forms, starting from weight pruning  \cite{DBLP:conf/nips/CunDS89} \cite{DBLP:conf/iclr/MolchanovTKAK17}\cite{han2016deepcompression} and
ending with knowledge distillation \cite{DBLP:journals/corr/HintonVD15} \cite{DBLP:conf/icmlt/OkadoMIKS22}, quantization presents itself as a promising 
area especially in the sense of learned quantization - 
the process of making ML models learn their optimal quantization
on their own.
Hence, this work introduces two techniques designed to address the primary challenge of learned quantization:
achieving effective model compression without incurring significant accuracy degradation.
While the first technique involves custom loss functions that directly take into account
quantization goals, the second, more novel approach incorporates a custom scaling factor gradient calculation that takes into account 
the gradient of the parameters that are being quantized.
As a result, a memory usage reduction of up to \( 8 \times \) is achieved for the parameters of networks
trained on MNIST, CIFAR-10, and Imagenette. 