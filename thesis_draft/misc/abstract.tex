\thispagestyle{empty}
\vspace*{1.0cm}

\begin{center}
    \textbf{Abstract} \label{abstract}
\end{center}

\vspace*{0.5cm}

\noindent% State the problem
Machine learning (ML) models are notoriously resource-intensive.
Given their widespread application on end-user devices,
the need to reduce the memory and computing requirements of such models is becoming ever more pressing.
% Say why it's an interesting problem
Interestingly, the very redundancy that contributes to the resource-intensive nature of predictive models, 
offers the remedy to this malady by creating opportunities for exploitation.
While redundancy exploitation in ML models has been explored in various forms,
including weight pruning \cite{DBLP:conf/nips/CunDS89} \cite{DBLP:conf/iclr/MolchanovTKAK17}\cite{han2016deepcompression},
knowledge distillation \cite{DBLP:journals/corr/HintonVD15} \cite{DBLP:conf/icmlt/OkadoMIKS22} and ...,
quantization presents itself as a promising 
area especially in the sense of learned quantization - 
where ML models are designed to learn their optimal quantization strategies autonomously.
% Say what your solution achieves
Hence, this work introduces two techniques for learned quantization,
achieving effective model compression without incurring significant accuracy degradation.
The first technique involves custom loss functions that directly take into account
quantization goals, the second novel approach incorporates a custom scaling factor gradient calculation that utilizes 
the gradient of the parameters that are being quantized.
As a result, we achieve a memory usage reduction of up to \( 8 \times \) is achieved for the model weights
trained on MNIST, CIFAR-10, and Imagenette, a simplified ImageNet dataset. 