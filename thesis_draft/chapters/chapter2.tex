\chapter{Background\label{cha:chapter2}}

This chapter addresses the theoretical and contextual background 
necessary to understand the key concepts and methodologies 
that form the foundation of the current research.
The first section will discuss the basics of deep NNs, 
upon which the technical setup of this thesis is based.
The next section aims to provide a broader context for the term quantization, 
followed by a final section that explains common techniques of \textit{learned quantization}, 
as well as the trade-offs and challenges they present. 


% ------------------------------------------------------------
% ----------------------- deeplearning ----------------------- 
% ------------------------------------------------------------
\section{Fundamentals of Deep Learning}
\label{sec:deeplearning}
This section introduces the fundamental concepts of deep learning, 
beginning with the most basic NN architecture components \ref{subsec:denseconvolutional} and progressing to loss functions with regularization \ref{subsec:lossregularization}. 
The concepts of the forward pass and backpropagation will be explained in the last subsection \ref{subsec:forwardback}.

% -------------------- denseconvolutional --------------------
\subsection{Dense and Convolutional Layers}
\label{subsec:denseconvolutional}
NNs can be considered a mathematical abstraction of the human decision-making process. 
Consider a scenario where, given an image, you need to say aloud what you see. 
The two eyes can be regarded as input nodes that receive the initial data, 
the brain can be seen as a set of \textit{hidden layers} that process this data, 
and your mouth — the output node that provides the final answer.
\\
\\
A hidden layer, which typically consists of many neurons, is where the magic
 — or the transformation of data — happens. In its simplest form, 
 within the classic \textit{Multilayer Perceptron} (MLP) model,
 each hidden layer neuron performs a weighted operation:

\[
\textit{output} = f(w \cdot \textit{input} + b)
\]
\\
\noindent where:

\begin{itemize}
  \item \textit{input} refers to the outputs from the previous layer (or the initial data from input nodes) that are fed into a specific neuron in the hidden layer.
  \item \textit{w} (weights) is a vector of parameters associated with that specific neuron, defining the importance of each input received by this neuron. 
  \item \textit{b} (bias) is an additional scalar parameter specific to the neuron, which shifts the result of the weighted sum, allowing for more flexibility.
  \item \textit{f} is the \textit{activation function}, a nonlinear function applied to the weighted sum of inputs and bias in that specific neuron, allowing for more complexity.
  \item \textit{output} is the result produced by the neuron, which will then be passed on to the next hidden layer (or to the final output layer).
\end{itemize}

\noindent Hidden layers where each neuron is connected to every neuron in the previous layer 
and every neuron in the next layer are called \textit{dense layers}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=14cm]{dense_layer}
  \caption{An example of a NN with two hidden dense layers, showing the connections between neurons in adjacent layers.}
  \label{fig:dense_layer}
\end{figure}

\noindent Mathematically, dense layers can be represented as:

\[
\textit{a} = f(W \cdot \textit{x} + b)
\]
\\
\noindent where:
\begin{itemize}
  \item \textit{x} is the input vector, representing outputs from the previous layer (or initial input data for the first layer).
  \item \textit{W} is the \textit{weight matrix}, with each row corresponding to the weight vector \( w \) of a specific neuron.
  \item \textit{b} represents the bias vector, where each scalar element corresponds to the bias of a specific neuron.
  \item \textit{f} is the \textit{activation function} that is applied element-wise.
  \item \textit{a} refers to the output vector, representing the activations of all neurons.
\end{itemize}

\noindent This interconnectedness of dense layers introduces the inherent redundancy, 
or the over-parameterizedness of NNs \cite{gholami2021survey}. It is particularly true in models with a large number of neurons, 
where \( W \) results in a vast number of parameters, which do not contribute to the model accuracy equally \cite{hubara2016qnn}.
\\
\\
\noindent \textit{Convolutional layers} are another type of hidden layers 
that involve a \textit{convolution} operation on the input. Intuitively, a standard convolution is 
a process of sliding a small grid over an input to find patterns. The figure below, for example, shows 
the application of the Sobel kernel that detects edges on the input image.

\begin{figure}[h!]
  \centering
  \includegraphics[width=14cm]{convolution}
  \caption{A 3×3 kernel (filter) sliding over a padded input matrix to compute the output feature map, demonstrating the interaction between the kernel weights and input values at a specific position.}
  \label{fig:convolution}
\end{figure}

\noindent For multi-channeled inputs, like RGB images, the convolution operation uses a multi-channeled kernel, as shown in Figure 2.3, 
producing a single-channeled feature map that combines weighted contributions from all input channels. 
A convolutional layer typically includes multiple such kernels, generating feature maps equal to the number of kernels.
After the convolution operation generates the feature maps, a bias term is added to each map, 
and the activation function is applied element-wise — just like in dense layers.

\begin{figure}[h!]
  \centering
  \includegraphics[width=14cm]{convolution_multiple_channels.png}
  \caption{A 3×3x3 kernel (filter) sliding over an RGB input matrix to produce a single-channeled output feature map.}
  \label{fig:convolution_multiple_channels}
\end{figure}

\noindent Mathematically, a convolutional layer can be represented as:

\[
y_{i,j,k} = \phi \left( \sum_{m=1}^M \sum_{p=1}^P \sum_{q=1}^Q x_{i+p-1, j+q-1, m} \cdot w_{p,q,m,k} + b_k \right)
\]
\\
\noindent where:
\begin{itemize}
  \item \( P, Q \) are the height and width of the filter, respectively.
  \item \( M \) is the number of input channels.
  \item \( y_{i,j,k} \) denotes the output at position \((i, j)\) for the \(k\)-th filter.
  \item \( x_{i+p-1, j+q-1, m} \) is the input at position \((i+p-1, j+q-1)\) for the \(m\)-th input channel.
  \item \( w_{p,q,m,k} \) represents the weight of the filter at position \((p, q)\) for the \(m\)-th input channel and \(k\)-th filter.
  \item \( b_k \) is the bias for the \(k\)-th filter.
  \item \( \phi(\cdot) \) is the activation function.
\end{itemize}

\noindent In simpler terms, a convolutional layer applies filter weights 
as it slides over rows \((p)\), columns \((q)\), and channels \((m)\), 
sums the results, adds bias \((b_k)\), 
and repeats this for all positions \((i, j)\) and filters \((k)\).
\\
\\
\noindent Although convolutional layers often have fewer weight parameters than dense layers in typical architectures, 
they still contain redundancies \cite{huang2017densely}, presenting an opportunity for quantization. 
Thus, both dense and convolutional layers will be the focus of this work.

% -------------------- lossregularization --------------------

\subsection{Loss Functions and Regularization}
\label{subsec:lossregularization}
The weights and biases are usually \textit{learnable parameters}
that the model adjusts during \textit{training}.
The training process of NNs is similar to how our brains learn from mistakes. 
Given the ground truth, a NN adjusts its learnable parameters 
using a specific function that compares the ground truth with the output generated by the network, 
essentially measuring the magnitude of the network's errors.
\\
\\
This function is called a \textit{loss function}, and depending on the type of question the network aims to answer, it can take many different forms.
For example, for the MLP described in Figure~\ref{fig:dense_layer} that generates a binary classification, we would use the \textit{log loss} function. 
Since the datasets used in this thesis involve multi-class classification, the \textit{sparse categorical cross-entropy} (SCCE) loss function will be used, 
which measures the difference between the predicted class probabilities and the true labels for each class in the dataset.
\\
\\
Often the loss function alone is not enough for a NN to perform well, 
as it may lead to overfitting or fail to capture desired generalization properties.
This is why a \textit{regularization term} that penalizes unwanted behaviours
is added to the loss function.
\\
\\
A typical regularization term is \( L_2 \), 
which penalizes large weights by adding the sum of the squared weights to the loss. 
The modified loss function is then expressed as:

\[
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{data}} + \lambda \sum_{i} w_i^2
\]
\\
\noindent where:

\begin{itemize}
  \item \( \mathcal{L}_{\text{data}} \) is the original loss function (in our case, the SCCE loss function).
  \item \( \lambda \) is a scalar parameter that controls the strength of the regularization.
  \item \( w_i \) represents each individual weight value in the model.
\end{itemize}

\noindent The current work employs multiple custom regularization terms 
that encourage specific behaviors in the models while discouraging others. 
These terms will be discussed in detail in the Experimental Setup section \ref{sec:setup}.

% -------------------- forwardback --------------------

\subsection{Forward-Pass and Back-Propagation}
\label{subsec:forwardback}
The repetition of the mathematical operations described earlier in the Dense and Convolutional Layers subsection \ref{subsec:denseconvolutional} 
during model training constitutes the \textit{forward pass}. 
It is the process where input data is passed through the network layer by layer, 
with each layer applying its learned weights and biases to produce a final output. 
\\
\\
As mentioned in the previous subsection, this output is then compared with the ground truth by the loss function that produces an error.
This error is used to update the learnable parameters in \textit{W} and \textit{b} during a process called \textit{back-propagation}.
\\
\\
In other words, back-propagation is the method by which the network adjusts its parameters to minimize the error. 
It calculates the gradient of the loss function with respect to each parameter using the chain rule. 
\( W \) and \( b \) are typically updated as follows:

\[
W = W - \eta \frac{\partial L}{\partial W}, \quad b = b - \eta \frac{\partial L}{\partial b}
\]
\\
\noindent where \( L \) is the loss function, and \( \eta \) is the learning rate.
\\
\\
For example, consider the weight \( w_{1,1} \) represented as the line between \( x_1 \)
 and the hidden layer node \( {a_1}^{(1)} \) in Figure~\ref{fig:dense_layer}. 
The gradient of this weight with respect to the loss is calculated using the chain rule:

\[
\frac{\partial L}{\partial w_{1,1}} = \frac{\partial L}{\partial o_1} \cdot \frac{\partial o_1}{\partial {a_1}^{(1)}} \cdot \frac{\partial {a_1}^{(1)}}{\partial w_{1,1}}
\]

\noindent Where:
\begin{itemize}
    \item \( \frac{\partial L}{\partial o_1} \) is the gradient of the loss with respect to \( o_1 \).
    \item \( \frac{\partial o_1}{\partial {a_1}^{(1)}} \) is the gradient of \( o_1 \) with respect to the output of \( {a_1}^{(1)} \).
    \item \( \frac{\partial {a_1}^{(1)} }{\partial w_{1,1}} \) is the value of  \( x_1 \), since  \( {a_1}^{(1)} \) is a weighted sum of the inputs.
\end{itemize}

\noindent This shows how each weight contributes to the final error during back-propagation.

% ------------------------------------------------------------
% ----------------------- basicsofquantization ----------------------- 
% ------------------------------------------------------------

\section{Basics of Quantization}
\label{sec:basicsofquantization}
This section aims to answer the \textit{why} question with respect to quantization and further provides a broader understanding of the term regarding its types.

% -------------------- purpose and definition --------------------

\subsection{Purpose and Definition}
\label{subsec:purposeanddefinition}
As we become increasingly dependent on deep learning models disguised as everyday tools, 
the need for these models to function in a resource- and time-efficient manner is more imperative than ever. 
The focus on resource efficiency is particularly important, 
with the research community expressing concerns regarding the environmental effects of large models, 
the exponential size growth of which continues to significantly outpace that of system hardware \cite{DBLP:journals/corr/abs-2111-00364}. 
In this regard, studies have examined quantization within the context of Green AI as a method to reduce the carbon footprint of
ML models \cite{DBLP:journals/csi/RegueroMV25}.
\\
\\
Aside from the environmental considerations, the mere need to reduce 
the computational cost and speed of predictive models
comes as an apparent business requirement. 
This requirement is essential when — quite ironically — embedded systems, famous for their compactness, meet 
ML models, infamous for their complexity. Microcontrollers, for instance, 
usually are not able to perform floating-point operations, which must therefore be emulated in software, 
introducing significant overhead. This is why quantization, the process which reduces the memory footprint of a model,
is also extensively covered in the realm of embedded systems that 
inherently prefer integer arithmetic, as well as bitwise operations \cite{rastegari2016xnor} \cite{DBLP:conf/eccv/ZhangYYH18} \cite{DBLP:conf/codit/KhalifaM24}\cite{DBLP:journals/corr/abs-2105-13331}.
\\
\\
Another motivation for quantization — although somewhat controversial — is the fact that reducing the bit-width of
ML models makes them robust to adversial attacks in certain cases \cite{DBLP:journals/corr/abs-2404-05639}.
This holds significant value in fields, such as autonomous driving,
where model vulnerability may result in fatal outcomes.
Interestingly enough, the use cases where such robustness is required also demand fast inference, 
as they rely on real-time predictions. Consider healthcare diagnostics needed for emergency scenarios 
or military defense mechanisms designed for immediate action.
\\
\\
The list of reasons why quantization is useful may go on for a while, but regardless of the motivation,
the essence of the term itself — rooted in the early 20th century — remains unchanged:
quantization refers to the division of a quantity into a discrete number
of small parts \cite{gray1998quantization}. With regard to ML models, 
it describes the process of dividing higher bit-width numbers into a discrete number of lower bit-width representations
without causing significant degradation in performance \cite{gholami2021survey}.
\\
\\
Since ML models are generally considered redundant or over-parameterized,
there are multiple points where quantization can be applied.
Specifically, in this thesis, we apply quantization to the weights and biases of dense layers, 
as well as the kernels and biases of convolutional layers. 
Other applications include, but are not limited to, layer activation and layer input quantization (two sides of the same coin),
as well as gradient quantization. The bottom line is that wherever there is an opportunity for arithmetic or memory optimization,
there is room for quantization.

% -------------------- Common Quantization Approaches --------------------

\subsection{Core Quantization Approaches}
\label{subsec:commonquantizationapproaches}

There is a multitude of ways to classify NN quantization methods, a broader overview of which will be covered
in the Related Work chapter \ref{cha:chapter5}.
For now, we will focus on a few basic approaches from the general categories of both 
\textit{data-driven} and \textit{data-free} methods \cite{Edouard2022SPIQ} to provide a basic understanding of the NN quantization process.
\\
\\
The simplest form of data-free quantization, or \textit{post-training quantization} \cite{jiang2021efficient},
involves converting already trained parameters from FP32 to a lower bit-width format
without using the initial training data. 
A common approach is to apply \textit{uniform quantization} that maps real values to a set number
of \textit{bins}. The general formula can be written as:

\[
Q(r) = round(\frac{r}{S})
\]

\noindent Where:
\begin{itemize}
    \item $Q(\cdot)$ denotes the quantization operation.
    \item $r$ is the real value of a given model parameter in higher bit-width representation.
    \item $round(\cdot)$ is some rounding operation, such as a simple $floor(\cdot)$.
    \item $S$ is a scaling factor.
\end{itemize}

\noindent  As a result, we essentially end up with a discrete number of values in lower bit precision, 
instead of an almost continuous range of real numbers as shown in Figure \ref{fig:quantizing_example}.
\\
\begin{figure}[h!]
  \centering
  \includegraphics[width=10cm]{quantizing_example.png}
  \caption{An example illustrating the quantization operation on the weight matrix from Figure \ref{fig:dense_layer}, 
  with arbitrary values for demonstration purposes.}
  \label{fig:quantizing_example}
\end{figure}

\noindent Unlike data-free quantization  —  as the name suggests — data-driven quantization typically involves retraining the model
using the initial data. An example of this approach is the Ristretto framework \cite{DBLP:journals/tnn/GyselPMG18}, which, similar to data-free methods, 
first analyzes the trained model to select suitable lower bit-width number formats for its weights.
Then, using a portion of the original dataset, the framework determines appropriate formats for layer inputs and outputs.
As a next step, based on the validation data, Ristretto adjusts the quantization settings to achieve optimal performance 
under the given constraints. Finally, the quantized model is fine-tuned using the training data.
\\
\\
A much simpler example of data-driven quantization could be the min-max quantization on input data as shown in Figure \ref{fig:min_max_quantization}. 
This method can also be used in a data-free scenario to quantize learned model parameters and is internally used as one of the
default techniques in popular ML frameworks like Tensorflow and PyTorch.

\begin{figure}[h!]
  \centering
  \includegraphics[width=15cm]{min_max_quantization.png}
  \caption{ An example illustrating min-max quantization of input data to 8 bits, followed by matrix multiplication with the quantized weight matrix from Figure \ref{fig:quantizing_example}.
  Input data has arbitrary values for demonstration purposes.}
  \label{fig:min_max_quantization}
\end{figure}

\noindent In the previous subsection, we discussed \textit{where} quantization could be applied in a model, 
mentioning weights, kernels, and biases as the focus of this thesis.
Figures \ref{fig:quantizing_example} and \ref{fig:min_max_quantization} show examples of quantization
using a scalar scaling factor. However, scaling factors could be applied at varying levels of detail,
and this is where the concept of \textit{quantization granularity} comes into play.
\\
\\
Granularity refers to the level of detail at which scaling factors are applied,
ranging from a single factor for an entire kernel (coarse granularity) to separate factors for individual spatial locations,
channels, or filters (fine granularity). For instance, Figure \ref{fig:granularity-conv2d} illustrates various possible granularities
for the kernels of convolutional layers.
Despite this wide range of possibilities, channel-wise quantization is currently the standard for convolutional layers \cite{gholami2021survey},
as it helps parallel processing capabilities of accelerators that compute channel outputs independently. 
For dense layers, row-wise quantization (one scaling factor for weights used by a single output neuron) is more prevalent
because it aligns with matrix-vector multiplication, which then can be carried out by specialized linear algebra libraries
in an optimized way \cite{DBLP:journals/corr/abs-2101-05615}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=15cm]{granularity-conv2d.png}
  \caption{A demonstration of the varying application of scaling factors, ranging from a single scalar applied to the entire kernel (1) to separate scalars assigned to spatial dimensions (e.g., 1, 2), channels (4), filters (9), and other granular configurations.}
  \label{fig:granularity-conv2d}
\end{figure}



% ------------------------------------------------------------
% ----------------------- learnedquantization ----------------------- 
% ------------------------------------------------------------
\section{Learned Quantization}
\label{sec:section3}
Now that the fundamentals of quantization have been covered, 
this section introduces key concepts commonly encountered in learned quantization,
including its challenges, trade-offs, and the popular techniques used to overcome them.


% -------------------- tradeoffs and challenges --------------------

\subsection{Trade-offs and Challenges}
\label{subsec:subsection1}
The inherent — or rather the generally accepted — characteristic of quantization is
that it negatively influences performance. 
This is referred to as the trade-off between quantization and generalization, 
which reflects the question of how much accuracy — or whatever performance metric is being used — 
we are willing to sacrifice to gain a reduction in computational cost, memory usage, or inference time.
However, the truth is that we usually cannot afford sacrificing anything.  This, coupled with the lack of a guarantee
 that pre-defined \textit{quantizers} can yield optimal results \cite{DBLP:conf/eccv/ZhangYYH18} \cite{DBLP:conf/iclr/EsserMBAM20}, 
 has paved the way for the burgeoning field of learned quantization,
 which aims to \textit{learn} how to quantize the model in a manner that mitigates performance loss.
\\
\\
Learned quantization is, however, a double-edged sword in the sense that, despite producing compact results,
the cost to achieve them is higher \cite{DBLP:conf/eccv/ParkYV18}. The obvious reason is the additional computational overhead
introduced by learnable quantizers.
Thus, it is important to strike a balance between learning optimal quantization and keeping the training process manageable
— which explains the prevailing emphasis on simplicity in most learned quantization research.
\\
\\
The main issue in achieving this simplicity is posed by the fact that discretization, in its essence, is non-differentiable
 — meaning it is challenging to integrate any kind of discretizing operations into gradient-based optimization methods, 
 upon which ML models rely. Using the chain rule back-propagation example from subsection \ref{subsec:forwardback}, let's
consider a simple flooring operation introduced into the process to better understand the problem.
\\
\\
Suppose we want to quantize activations and apply \( floor(\cdot) \) to hidden layer outputs:
\[
  {a_{1,q}}^{(1)} =  floor({a_1}^{(1)})
  \]
As a result, the chain rule becomes:
 \[
  \frac{\partial L}{\partial w_{1,1}} =  \frac{\partial L}{\partial {a_{1,q}}^{(1)}} 
  \cdot \frac{\partial {a_{1,q}}^{(1)}}{\partial {a_1}^{(1)}} 
  \cdot \frac{\partial {a_1}^{(1)}}{\partial w_{1,1}}
  \]
  
  \noindent where \( \frac{\partial {a_{1,q}}^{(1)}}{\partial {a_1}^{(1)}} \) presents a challenge. 
  Since \(  {a_{1,q}}^{(1)} =  floor({a_1}^{(1)}) \), the derivative is: 
            \[
            \frac{\partial a_{1,q}^{(1)}}{\partial a_{1}^{(1)}} =
            \begin{cases} 
                0 & \text{if } a_{1}^{(1)} \notin \mathbb{Z}, \\
                \text{undefined} & \text{if } a_{1}^{(1)} \in \mathbb{Z}.
            \end{cases}
            \]
  This means that for most values of \( {a_1}^{(1)} \), which are non-integer, the gradient becomes $0$, resulting in 
  $w_{1,1}$ not receiving any updates. For integer values of \( {a_1}^{(1)} \), the backpropagation process fails altogether.
\\
\\
In essence, circumventing the issue of non-differentiability is the fundamental problem that learned quantization aims to solve,
all while managing the aforementioned trade-offs to produce a model that is compact, not overly complex to train,
and highly performant.

% -------------------- common methods --------------------

\subsection{Common Methods}
\label{subsec:commonlearnedquantizationmethods}

The most common method to address the issue of non-differentibility is to approximate the gradient of quantization operators
using the Straight-Through Estimator (STE) \cite{bengio2013estimating} \cite{fan2021training}. This workaround applies the quantizion operation as is 
during the forward-pass, but replaces the gradient of the piece-wise discontinuous function 
with that of a continuous identity function. Returning to the example from the previous subsection,
if we apply the STE to the problematic gradient, instead of:
\[
  \frac{\partial a_{1,q}^{(1)}}{\partial a_{1}^{(1)}} =
  \begin{cases} 
      0 & \text{if } a_{1}^{(1)} \notin \mathbb{Z}, \\
      \text{undefined} & \text{if } a_{1}^{(1)} \in \mathbb{Z}.
  \end{cases}
  \]
we approximate it as: 
\[
  \frac{\partial a_{1,q}^{(1)}}{\partial a_{1}^{(1)}}  \approx 1
  \]
This enables gradient flow, allowing model parameters to receive updates
without being hindered by the non-differentiability of the quantization step.
\\
\\
The STE — and other estimators \cite{DBLP:journals/jstsp/Chen0ZHY20} —  are the cornerstone of quantization-aware training (QAT) \cite{jacob2018quantization},
a subfield that falls under the broader umbrella of learned quantization.
QAT, however, does not necessarily focus on learning quantization parameters.
Instead, it focuses on helping the model adapt to the loss caused by the quantization process during the forward pass,
whether or not trainable quantizers are involved.
\\
\\
More often than not QAT and trainable quantization parameters are combined. 
An example is quantization-interval-learning (QIL) \cite{DBLP:conf/cvpr/JungSLSHKHC19}, 
which uses three trainable quantization parameters 
(the center of the interval, the distance to the center, and a parameter that controls the scaling itself) 
with piecewise differentiable operations and relies on STE for gradient updates of quantized weights and activations. 
Similarly, the learned step size quantization (LSQ) approach \cite{DBLP:conf/iclr/EsserMBAM20} 
defines a single trainable quantization parameter (step size) with an explicit gradient formula and also uses STE for standard parameter updates.
LQ-Nets \cite{DBLP:conf/eccv/ZhangYYH18} depend on STE too, but  —  unlike the two previous techniques — 
directly incroporate a quantization error minimazation algorithm to calculate binary encodings and the associated bases of
model parameters given a predefined bit size. 
\\
\\
INSERT STUFF ABOUT QUANTIZATION ENCOURAGING REGULARIZATION
\\
\\
In this thesis, we will utilize the STE but introduce a custom gradient calculation for the scale factor gradients
based on a specific type of gradient sensitivity in the model parameters.
Additionally, we will explore custom loss regularization terms that encourage quantization
and systematically compare them across different datasets.