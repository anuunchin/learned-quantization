\chapter{Introduction\label{cha:chapter1}}

Such is the life of a modern human being that not a single day passes 
without a machine learning model toiling away in the background. 
From unlocking one's phone with Face ID in the morning 
to receiving a curated recommendation feed on Netflix in the evening — 
all is ML — but at what cost?
\\
\\
If we consider GPT-3 as an example, 
its 175 billion parameters need a whopping 700 gigabytes of storage in total —  
4 bytes for each parameter represented in single-precision floating-point format (FP32).
This costliness of modern ML models has revitalized interest in the research area
of \textit{quantization of Neural Networks} (NNs) 
which aims to reduce model size by developing methods 
that directly or indirectly decrease the amount of memory 
needed to store parameters numbering in the millions or billions. 
Going back to the GPT-3 example, by directly clamping its FP32 parameters 
to an 8-bit integer (INT8) representation, we can reduce its storage requirement 
from 700 to just 175 gigabytes. 
\\
\\
But what costs does quantization itself entail? The natural answer to this question 
would be a reduction in model performance, as a decrease in precision logically implies 
a decrease in accuracy. However, there is a somewhat counterintuitive phenomenon where 
quantization improves accuracy by introducing noise, which act like a
form of regularization, forcing the model to generalize better \cite{courbariaux2015binaryconnect}. No matter whether quantization
results in better or a slightly worse performance, the assumptions is that 
for each model there is an optimal way to quantize it within reasonable degradation ranges.
And if the model is able to learn its optimal parameters, 
it is most likely also able to learn its optimal quantization parameters. 
\\
\\
The fact that, indeed, models can learn their optimal quantization parameters has been proven 
many times in the past. But is there a way to make them do it better - is a question 
that will always remain and to which this thesis aims to contribute. In that sense, 
the current work will explore novel ways to tackle the two main problems that the process 
of learning optimal quantization parameters poses. First, how to overcome the issue 
of non-differentiability of rounding operations in the back-propagation. 
Second, how to guide the model to quantize selectively where necessary
and adaptively relax the quantization when a certain threshold is reached. 
\\
\\
<The following paragraph will be replaced - cause written in a very confusing way>
\\
\\
While there is a number of methods dealing with the issue of non-differentiability of rounding operations
in quantization, the most popular of them are the Straight-Through Estimator and other similar approaches 
that employ continuous relaxations of discrete functions to enable gradient-based optimization. 
Besides these approximation methods, other more aggressive techniques — in a sense — avoid 
non-differentiability altogether and perform quantization based on strictly defined constraints. 
Binary Connect \cite{courbariaux2015binaryconnect}, for example, binarizes weights during both forward pass and backpropagation, 
with the real values of weights used only during parameter update. 
However, these aggressive methods, while effective in simpler scenarios, 
often struggle on more complex datasets. The more effective methods seem to combine both constraints 
and gradient approximations — just like XNOR-Net enhances BinaryConnect with a 
gradient computation formula designed for binary weights \cite{rastegari2016xnor}. 
\\
\\
Despite the abundance of different methods, there is still room for improvement with regard to the second problem mentioned earlier,
namely, not only how to quantize, but also where to do so and when to stop.
Therefore, in this thesis, we try to fill this room with the following contributions:
\begin{itemize}
    \item We introduce a method that directly considers the gradient-to-parameter ratio,
    which conveys how much the parameter is being adjusted relative to its current value.
    Based on this ratio, the model learns its quantizer scaling factors, applied at different granularities,
    alongside the standard trainable parameters.
    \item We also provide a modular framework that can be easily integrated into a wide range of applications and layers
    with minimal adjustments, ensuring flexibility and usability.
    \item We additionaly provide appliccable ranges for this ratio for effective quantizaiton for dense and convolutional layers.
    \item <The loss function terms...>
\end{itemize}
 